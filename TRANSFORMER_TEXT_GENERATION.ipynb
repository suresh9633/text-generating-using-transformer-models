{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fantasy Story Generation using Transformer Models: A Project Report\n",
    "\n",
    "This report outlines the process of developing a text generation model capable of producing creative short stories in the fantasy genre by fine-tuning a pre-trained GPT-2 transformer model."
   ],
   "metadata": {
    "id": "intro_md"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "--- \n",
    "## 1. Project Scope and Goals\n",
    "\n",
    "The first step is to define the project's objectives, target audience, and constraints."
   ],
   "metadata": {
    "id": "scope_goals_md"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scope_goals_code"
   },
   "outputs": [],
   "source": [
    "project_goal = \"\"\"\n",
    "The primary goal of this project is to develop a text generation model capable of producing high-quality, creative short stories in the fantasy genre.\n",
    "\"\"\"\n",
    "\n",
    "text_type = \"Creative short stories in the fantasy genre.\"\n",
    "\n",
    "target_audience = \"\"\"\n",
    "Writers and creative individuals seeking inspiration or assistance in generating story ideas and narratives.\n",
    "\"\"\"\n",
    "\n",
    "constraints_requirements = \"\"\"\n",
    "-   **Computational Resources:** Training will be performed on cloud-based GPUs (e.g., AWS Sagemaker, Google Colab Pro). A limited budget requires efficient model architecture selection and training strategies.\n",
    "-   **Time Limitations:** Project completion within 3 months. This includes data collection, model selection, training, evaluation, and documentation.\n",
    "-   **Ethical Considerations:** The generated text should not contain harmful, biased, or offensive content. Mechanisms for filtering or mitigating such outputs will be necessary.\n",
    "-   **Data Availability:** Sourcing a diverse and high-quality dataset of fantasy short stories might be challenging due to copyright and accessibility. Focus on publicly available datasets or creative commons licensed works.\n",
    "-   **Model Complexity:** The chosen model should be complex enough to capture the nuances of creative writing but not so complex that it becomes computationally infeasible to train within the given time and resource constraints.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Project Goal:\")\n",
    "print(project_goal)\n",
    "print(\"\\nType of Text to be Generated:\")\n",
    "print(text_type)\n",
    "print(\"\\nTarget Audience:\")\n",
    "print(target_audience)\n",
    "print(\"\\nConstraints and Requirements:\")\n",
    "print(constraints_requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 2. Data Collection and Preprocessing\n",
    "\n",
    "A suitable dataset is essential for training the model. The process involves gathering, cleaning, and preparing the data for the model.\n",
    "\n",
    "### 2.1. Data Collection (Simulated)\n",
    "\n",
    "Due to copyright and accessibility challenges, sourcing a large dataset of fantasy stories can be difficult. For this project, we simulate a small sample dataset to demonstrate the workflow."
   ],
   "metadata": {
    "id": "data_collection_md"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_collection_code"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simulate a sample dataset of fantasy short stories\n",
    "# In a real project, this would involve web scraping, using APIs, or downloading datasets\n",
    "sample_data = {\n",
    "    'story_id': [1, 2, 3],\n",
    "    'story_text': [\n",
    "        \"In the ancient forest, where shadows danced and whispers echoed, lived Elara, an enchantress of unparalleled power. Her eyes, the color of the deepest forest pool, held secrets older than the mountains. One day, a quest arrived...\",\n",
    "        \"A lone knight, clad in silver armor, stood before the towering gates of the Dragon's Lair. Sir Kaelen had faced countless foes, but none as fearsome as the beast that guarded the lost artifact. The air crackled with magic...\",\n",
    "        \"Deep beneath the earth, the dwarves of Ironpeak delved for mithril. Their hammers rang against stone, a rhythm as old as time. Among them was Borin, a young dwarf with dreams of adventure beyond the mines. A hidden passage awaited...\"\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "print(\"Sample Data:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2. Data Cleaning\n",
    "\n",
    "The raw text data is cleaned to remove special characters and convert it to a consistent format."
   ],
   "metadata": {
    "id": "data_cleaning_md"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_cleaning_code"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and punctuation (keeping spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['cleaned_story_text'] = df['story_text'].apply(clean_text)\n",
    "\n",
    "print(\"Cleaned Data:\")\n",
    "display(df[['story_id', 'cleaned_story_text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3. Data Splitting\n",
    "\n",
    "The dataset is split into training and testing sets. **Note:** With only 3 samples, this split (2 for training, 1 for testing) is insufficient for meaningful training but is included here to demonstrate the standard procedure."
   ],
   "metadata": {
    "id": "data_splitting_md"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_splitting_code"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.33, random_state=42)\n",
    "\n",
    "print(\"Train Set:\")\n",
    "display(train_df)\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 3. Model Selection and Setup\n",
    "\n",
    "We chose the GPT-2 model for this project due to its proven effectiveness in text generation and the extensive support provided by the Hugging Face ecosystem.\n",
    "\n",
    "### 3.1. Load Pre-trained Model and Tokenizer\n",
    "\n",
    "We load the pre-trained GPT-2 model and its corresponding tokenizer. The tokenizer's padding token is set to its end-of-sequence (EOS) token, which is standard practice for causal language models like GPT-2."
   ],
   "metadata": {
    "id": "model_selection_md"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_selection_code"
   },
   "outputs": [],
   "source": [
    "!pip install transformers[torch] datasets scikit-learn -q\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Choose and load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the pad_token_id to the eos_token_id for GPT-2\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2. Prepare Datasets for Training\n",
    "\n",
    "The cleaned text is tokenized and formatted into a Hugging Face `Dataset` object. A `labels` column, identical to `input_ids`, is added, which is necessary for the model to compute the language modeling loss during training."
   ],
   "metadata": {
    "id": "prepare_dataset_md"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_dataset_code"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Tokenize the cleaned story text\n",
    "def tokenize_function(examples):\n",
    "    # Use truncation and padding to handle varying sequence lengths\n",
    "    return tokenizer(examples[\"cleaned_story_text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "# Convert the pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[['cleaned_story_text']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['cleaned_story_text']])\n",
    "\n",
    "\n",
    "# Apply the tokenization function to the datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"cleaned_story_text\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"cleaned_story_text\"])\n",
    "\n",
    "# Add the 'labels' column for loss calculation\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True)\n",
    "tokenized_test_dataset = tokenized_test_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True)\n",
    "\n",
    "\n",
    "print(\"Training data prepared for GPT-2.\")\n",
    "print(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 4. Model Training\n",
    "\n",
    "The model is fine-tuned on our prepared fantasy story dataset. We use the `Trainer` class from the `transformers` library to handle the training loop."
   ],
   "metadata": {
    "id": "model_training_md"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_training_code"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-fantasy-stories\",      # Output directory for checkpoints and logs\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,                      # Number of training epochs\n",
    "    per_device_train_batch_size=1,           # Batch size (set to 1 due to small sample size)\n",
    "    save_steps=10_000,                       # Save checkpoint every 10,000 steps\n",
    "    save_total_limit=2,                      # Limit the total number of checkpoints\n",
    "    logging_steps=1,                         # Log every step\n",
    "    report_to=\"none\"                         # Disable external reporting\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 5. Model Evaluation\n",
    "\n",
    "After training, we evaluate the model's performance by generating text samples and manually assessing their quality.\n",
    "\n",
    "### 5.1. Generate Text Samples\n",
    "\n",
    "We create a function to generate stories from various prompts to see how the model performs."
   ],
   "metadata": {
    "id": "model_evaluation_md"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_evaluation_code"
   },
   "outputs": [],
   "source": [
    "def generate_story(model, tokenizer, prompt, max_length=150, num_return_sequences=1):\n",
    "    \"\"\"Generates text using the trained model.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2, # Helps prevent repetitive phrases\n",
    "        do_sample=True,         # Enable sampling for more creative output\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8,        # Control randomness\n",
    "    )\n",
    "\n",
    "    generated_stories = []\n",
    "    for i, output in enumerate(output_sequences):\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_stories.append(f\"--- Generated Story {i+1} ---\\n{text}\\n\")\n",
    "\n",
    "    return generated_stories\n",
    "\n",
    "# Generate text samples with different prompts\n",
    "prompts = [\n",
    "    \"In a hidden valley, a young wizard discovered a forgotten artifact\",\n",
    "    \"The ancient dragon awakened from its centuries-long slumber\",\n",
    "    \"Deep within the enchanted forest, a brave warrior sought the legendary sword\"\n",
    "]\n",
    "\n",
    "print(\"Generating text samples...\\n\")\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    generated_texts = generate_story(model, tokenizer, prompt)\n",
    "    for text in generated_texts:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2. Manual Assessment and Limitations\n",
    "\n",
    "The generated samples are manually reviewed for quality, coherence, and relevance.\n",
    "\n",
    "**Summary of Manual Assessment:**\n",
    "\n",
    "The generated text demonstrates a basic understanding of the prompts but is far from the goal of high-quality, creative storytelling.\n",
    "- **Quality:** The grammar is generally correct, but the sentences are simple.\n",
    "- **Coherence:** The stories often lack a clear narrative structure and can become nonsensical.\n",
    "- **Relevance:** The outputs stay on topic but fail to develop the initial idea in a meaningful way.\n",
    "\n",
    "**Limitations and Conclusion:**\n",
    "\n",
    "The poor performance is expected and is a direct result of the **extremely small training dataset**. A model cannot learn the complex nuances of creative writing from only two sample stories. To achieve the project's goal, a significantly larger and more diverse dataset is required.\n",
    "\n",
    "**Considerations for Future Quantitative Evaluation:**\n",
    "\n",
    "With a larger dataset, a more rigorous evaluation would include:\n",
    "1.  **Perplexity:** To measure the model's confidence on a test set.\n",
    "2.  **Human Evaluation:** The gold standard for creative tasks, where evaluators score stories on creativity, coherence, and fluency.\n",
    "3.  **Automated Metrics:** Using metrics like BLEU or ROUGE cautiously, as they are less suited for open-ended generation.\n",
    "4.  **Diversity Metrics:** To ensure the model generates varied and non-repetitive text."
   ],
   "metadata": {
    "id": "assessment_md"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 6. Model Deployment (Conceptual)\n",
    "\n",
    "Deploying the model would make it accessible for real-world applications. Below are potential options and considerations.\n",
    "\n",
    "**Potential Deployment Options:**\n",
    "* **Cloud Platforms:**\n",
    "    * **AWS Sagemaker, Google Cloud AI Platform, Azure ML:** Managed services that simplify deploying, scaling, and managing ML models.\n",
    "    * **Hugging Face Inference API/Endpoints:** A convenient and specialized service for hosting models from the Hugging Face Hub.\n",
    "* **Creating a Custom REST API:**\n",
    "    * Using frameworks like **Flask** or **FastAPI** to build a custom web service that provides full control over the model and infrastructure.\n",
    "\n",
    "**Factors Influencing Deployment Choice:**\n",
    "* **Traffic & Latency:** High-traffic, low-latency applications benefit from scalable cloud platforms.\n",
    "* **Budget:** Cloud platforms have a pay-as-you-go model, while self-hosting may be cheaper for consistent loads.\n",
    "* **Scalability & Expertise:** Cloud services offer easy scaling but may abstract away control. Custom APIs require more technical expertise to build and maintain.\n",
    "\n",
    "**General Deployment Steps:**\n",
    "1.  **Save** the final trained model and tokenizer.\n",
    "2.  **Package** the model, code, and dependencies (e.g., in a Docker container).\n",
    "3.  **Deploy** the package to the chosen platform (e.g., cloud endpoint, server).\n",
    "4.  **Test** the deployed endpoint to ensure it functions correctly.\n",
    "5.  **Integrate** the endpoint with the final application (e.g., a writer's tool).\n",
    "6.  **Monitor and Maintain** the model's performance and resource usage."
   ],
   "metadata": {
    "id": "deployment_md"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 7. Summary and Future Work\n",
    "\n",
    "### Summary\n",
    "This project successfully established a complete pipeline for fine-tuning a GPT-2 model for fantasy story generation. Key phases, including data preprocessing, model setup, training, and evaluation, were executed. However, the primary limitation was the severe lack of training data, which prevented the model from learning to generate high-quality, creative narratives. The manual evaluation highlighted this, showing outputs that were simplistic and lacked narrative depth.\n",
    "\n",
    "### Future Work\n",
    "The most critical next step is to **acquire a significantly larger and more diverse dataset** of fantasy stories. Once a substantial dataset is available, the following steps can be taken to improve the model:\n",
    "* **Implement a rigorous evaluation framework**, including both automated metrics and human evaluation, to accurately benchmark performance.\n",
    "* **Experiment with different model architectures** or larger models (e.g., GPT-2-medium, GPT-Neo) if resources permit.\n",
    "* **Explore advanced fine-tuning techniques** like LoRA (Low-Rank Adaptation) to potentially improve training efficiency and performance.\n",
    "* **Develop a user-friendly interface or API** to make the model accessible to the target audience of writers and creatives.\n",
    "* **Implement content filtering mechanisms** to ensure the generated text aligns with ethical guidelines and avoids harmful outputs."
   ],
   "metadata": {
    "id": "summary_future_work_md"
   }
  }
 ]
}